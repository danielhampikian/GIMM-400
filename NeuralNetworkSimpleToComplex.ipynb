{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetworkSimpleToComplex.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielhampikian/GIMM-400/blob/master/NeuralNetworkSimpleToComplex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8_n5sU5rtdt",
        "colab_type": "code",
        "outputId": "213587a6-8dcf-499b-9a97-5af615c2569c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# Simple Feedforward Artificial Neural Network: no external libraries like tensorflow, pandas, numpy, or keras:\n",
        "# No backpropogation or optimization:\n",
        "\n",
        "epoch = 5 #number of times you will run the neural net through the entire training set FIX THIS\n",
        "lr = .00001 #learning rate FIX THIS\n",
        "bias = .1 #value of bias FIX THIS\n",
        "weights = [random.random(),random.random(),random.random()] #weights generated in a list (3 weights in total for 2 neurons and the bias)\n",
        "print(\"Weights before training: \" + str(weights))\n",
        "\n",
        "def NeuralNet(weights, inputNeuron1, inputNeuron2, expectedOutput):\n",
        "  outputActual = inputNeuron1*weights[0]+inputNeuron2*weights[1]+bias*weights[2]\n",
        "  outputActual = ApplyActivation(outputActual)\n",
        "  error = expectedOutput - outputActual #simple error measurement\n",
        "  #inference: here we train the hidden layer which is just an array of weights.\n",
        "  weights[0] += error * inputNeuron1 * lr\n",
        "  weights[1] += error * inputNeuron2 * lr\n",
        "  weights[2] += error * bias * lr\n",
        "def ApplyActivation(actInput):\n",
        "  if actInput > 0 : #activation function (here Heaviside because we want a 0 or 1 answer)\n",
        "     actInput = 1\n",
        "  else :\n",
        "     actInput = 0\n",
        "  return actInput\n",
        "\n",
        "\n",
        "for i in range(epoch):\n",
        "  NeuralNet(weights, 1,1,1) #True or true input should return true\n",
        "  NeuralNet(weights, 1,0,1) #True or false input should return true\n",
        "  NeuralNet(weights, 0,1,1) #False or true input should return true\n",
        "  NeuralNet(weights, 0,0,0) #False or false input should return false\n",
        "  \n",
        "print(\"Weights after training: \" + str(weights))\n",
        "x = int(input())\n",
        "y = int(input())\n",
        "output = x*weights[0] + y*weights[1] + bias*weights[2]\n",
        "if output > 0 : #activation function\n",
        "   output = 1\n",
        "else :\n",
        "   output = 0\n",
        "print(x, \"or\", y, \"is : \", output)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights before training: [0.12417331511991114, 0.2791836790111395, 0.5857592714582879]\n",
            "Weights after training: [0.12417331511991114, 0.2791836790111395, 0.5857542714582877]\n",
            "0\n",
            "0\n",
            "0 or 0 is :  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEEZRWMWkeGe",
        "colab_type": "code",
        "outputId": "e1b5d687-5e89-48ca-a039-1a632da512dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        }
      },
      "source": [
        "import numpy as np #import numpy the linear algebra library\n",
        "\n",
        "# sigmoid function - introduces non-linearity into the neural net (not 1-1 input output)\n",
        "#sigmoid maps to a value between 0 and 1 to convert numbers to probabilities\n",
        "def nonlin(x,deriv=False):\n",
        "    if(deriv==True): # when true, this maps the derivative of the sigmoid function - so we can get the slope at a given point\n",
        "        return x*(1-x)\n",
        "    return 1/(1+np.exp(-x))\n",
        "    \n",
        "# input dataset\n",
        "X = np.array([  [0,0,1],\n",
        "                [0,1,1],\n",
        "                [1,0,1],\n",
        "                [1,1,1]])\n",
        "    \n",
        "# output dataset            \n",
        "y = np.array([[0,0,1,1]]).T\n",
        "\n",
        "# seed random numbers to make calculation\n",
        "# deterministic (just a good practice)\n",
        "np.random.seed(1)\n",
        "epochs = 10000\n",
        "weights = np.array([[0,0,0,0]]).T\n",
        "# initialize weights randomly with mean 0\n",
        "#we only have an input and output layer, so we only need on matrix of wieghts to connect them with dimensions (3,1) 3 inputs to 1 output\n",
        "#if we wanted to change the number of inputs we would also change this (3,1) to reflect inputs, but we generally want 1 output since we're doing a regresion neural net\n",
        "#syn0 = 2*np.random.random((3,1)) - 1\n",
        "\n",
        "def neuralNet(l0, epochs, predict, userTest=[0,0,0], testWeights=[0,0,0,0]):\n",
        "  if(not predict):\n",
        "    for iter in range(epochs):\n",
        "\n",
        "      # forward propagation - the prediction step, we let the network try to predict it's output and measure the error\n",
        "      syn0 = testWeights\n",
        "      #this is the 'weighted sum step where we use the dot or cross product of two matrices to output a matrix that has the multiplied and summed total of the weights and values for that row and column combination, and then we convert that back into a number since we get a probability with the sigmoid function'\n",
        "      #the dimensions going on in the next step are: (4X3)dot(3X1) = (4X1) so everything in the matrix gets multplied and hte result is a matix with the number of rows in first matrix and number of columns in second\n",
        "      l1 = nonlin(np.dot(l0,syn0)) #multiplies 10 input by syn0 weights, then passes it through our sigmoid function to convert the numbers to probabilities \n",
        "      \n",
        "\n",
        "      # compare our guess - l1 - to the actual answer - y -\n",
        "      l1_error = y - l1\n",
        "      if(iter % 999 == 0):\n",
        "        print('error before slope of sigmoid of values in l1')\n",
        "        print(l1_error)\n",
        "        print(\"l1 itself (dot product of l0 and weight (weighted sum))\")\n",
        "        print(l1)\n",
        "    \n",
        "\n",
        "      # multiply how much we missed by the \n",
        "      # slope of the sigmoid at the values in l1 - basically we multiply elementwise a 4,1 matrix with another 4,1 matrix of its sigmoid derivatives thereby reducing the error of high confidence predictions.\n",
        "      # if the network has a very confident guess (slope is very shallow or close to 0), we leave it alone with the multiplication here but if it's closer to .5 then we heavily update the guess with this multiplication\n",
        "      l1_delta = l1_error * nonlin(l1,True)\n",
        "      if(iter%999 == 0):\n",
        "        print('error after slope of sigmoid of values in l1 is multiplied times l1 error')\n",
        "        print(l1_delta)\n",
        "        \n",
        "    \n",
        "\n",
        "     # update weights (again cross multiplication , added to all weights between the two matrix)\n",
        "      syn0 += np.dot(l0.T,l1_delta)\n",
        "    return syn0,l1\n",
        "    \n",
        "  if predict:           \n",
        "          \n",
        "    syn0 = testWeights\n",
        "    total = 0    #the dimensions goig on in the next step are: (4X3)dot(3X1) = (4X1) so everything in the matrix gets multplied and hte result is a matix with the number of rows in first matrix and number of columns in second\n",
        "    for i in range(len(userTest)):\n",
        "      #multiplies 10 input by syn0 weights, then passes it through our sigmoid function to convert the numbers to probabilities \n",
        "    #print(\"hey\" + str(l1))\n",
        "      total += userTest[i] * testWeights[i]\n",
        "        # compare our guess - l1 - to the actual answer - y -\n",
        "        # l1_error = y - l1\n",
        "        #l1_delta = l1_error * nonlin(l1,True)\n",
        "        \n",
        "    \n",
        "\n",
        "      # update weights (again cross multiplication , added to all weights between the two matrix)\n",
        "        #syn0 += np.dot(l0.T,l1_delta)\n",
        "     # return the prediction:\n",
        "    return nonlin(total)\n",
        "      \n",
        "w,l1 = neuralNet(X,1000, False, testWeights = 2*np.random.random((3,1)) - 1)\n",
        "print(\"Output After Training:\")\n",
        "print(str(l1))\n",
        "print(\"Weights: \" + str(w))\n",
        "#user test:\n",
        "userInput = input(\"Enter int array: \")\n",
        "userArray = list(map(int, userInput.split()))\n",
        "print(userArray)\n",
        "print(str(neuralNet(X,1,True, userTest=userArray, testWeights=w)))\n",
        "#Note that adding a 0 0 0 learning set and a 0 1 0 doesn't work yet, we need to introduce two hyperparameters, layers, numbers of neurons, and an alpha or (learning rate)\n",
        "#Try 1 1 0 which is not in the data set, but if assume the rule it's trying to infer is perfect correspondence to the first column should return 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "error before slope of sigmoid of values in l1\n",
            "[[-0.2689864 ]\n",
            " [-0.36375058]\n",
            " [ 0.76237183]\n",
            " [ 0.6737243 ]]\n",
            "l1 itself (dot product of l0 and weight (weighted sum))\n",
            "[[0.2689864 ]\n",
            " [0.36375058]\n",
            " [0.23762817]\n",
            " [0.3262757 ]]\n",
            "error after slope of sigmoid of values in l1 is multiplied times l1 error\n",
            "[[-0.05289153]\n",
            " [-0.08418501]\n",
            " [ 0.13811206]\n",
            " [ 0.14809799]]\n",
            "error before slope of sigmoid of values in l1\n",
            "[[-0.03178421]\n",
            " [-0.02576499]\n",
            " [ 0.02093318]\n",
            " [ 0.02585355]]\n",
            "l1 itself (dot product of l0 and weight (weighted sum))\n",
            "[[0.03178421]\n",
            " [0.02576499]\n",
            " [0.97906682]\n",
            " [0.97414645]]\n",
            "error after slope of sigmoid of values in l1 is multiplied times l1 error\n",
            "[[-0.00097813]\n",
            " [-0.00064673]\n",
            " [ 0.00042903]\n",
            " [ 0.00065113]]\n",
            "Output After Training:\n",
            "[[0.03178421]\n",
            " [0.02576499]\n",
            " [0.97906682]\n",
            " [0.97414645]]\n",
            "Weights: [[ 7.26283009]\n",
            " [-0.21614618]\n",
            " [-3.41703015]]\n",
            "Enter int array: 0 1 1\n",
            "[0, 1, 1]\n",
            "[0.02575143]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO--FYBqxxwQ",
        "colab_type": "code",
        "outputId": "c9805ed2-c602-4de7-efad-adcdde307c6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def nonlin(x,deriv=False):\n",
        "\tif(deriv==True):\n",
        "\t    return x*(1-x)\n",
        "\n",
        "\treturn 1/(1+np.exp(-x))\n",
        "    \n",
        "X = np.array([[0,0,1],\n",
        "            [0,1,1],\n",
        "            [1,0,1],\n",
        "            [1,1,1]])\n",
        "                \n",
        "y = np.array([[0],\n",
        "\t\t\t[1],\n",
        "\t\t\t[1],\n",
        "\t\t\t[0]])\n",
        "\n",
        "np.random.seed(1)\n",
        "\n",
        "# randomly initialize our weights with mean 0\n",
        "syn0 = 2*np.random.random((3,4)) - 1\n",
        "syn1 = 2*np.random.random((4,1)) - 1\n",
        "\n",
        "for j in range(60000):\n",
        "\n",
        "\t# Feed forward through layers 0, 1, and 2\n",
        "    l0 = X\n",
        "  # It's really just 2 of the previous implementation stacked on top of each other. The output of the first layer (l1) is the input to the second layer.\n",
        "    l1 = nonlin(np.dot(l0,syn0))\n",
        "    l2 = nonlin(np.dot(l1,syn1))\n",
        "\n",
        "    # how much did we miss the target value?\n",
        "    l2_error = y - l2\n",
        "    \n",
        "    if (j% 10000) == 0:\n",
        "        print(\"Error:\" + str(np.mean(np.abs(l2_error))))\n",
        "        \n",
        "    # in what direction is the target value?\n",
        "    # were we really sure? if so, don't change too much.\n",
        "    l2_delta = l2_error*nonlin(l2,deriv=True)\n",
        "\n",
        "    # how much did each l1 value contribute to the l2 error (according to the weights)? This is a critical step!\n",
        "    # use the \"confidence weighted error\" from l2 to establish an error for l1. To do this, send the error across the weights from l2 to l1. \n",
        "    # This gives what you could call a \"contribution weighted error\" because we learn how much each node value in l1 \"contributed\" to the error in l2. This step is called \"backpropagating\" and is the namesake of the algorithm. We then update syn0 using the same steps we did in the 2 layer implementation.\n",
        "    l1_error = l2_delta.dot(syn1.T)\n",
        "    \n",
        "    # in what direction is the target l1?\n",
        "    # were we really sure? if so, don't change too much.\n",
        "    l1_delta = l1_error * nonlin(l1,deriv=True)\n",
        "\n",
        "    syn1 += l1.T.dot(l2_delta)\n",
        "    syn0 += l0.T.dot(l1_delta)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error:0.4964100319027255\n",
            "Error:0.008584525653247157\n",
            "Error:0.0057894598625078085\n",
            "Error:0.004629176776769985\n",
            "Error:0.0039587652802736475\n",
            "Error:0.003510122567861678\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9Y_QGaC6rSA",
        "colab_type": "code",
        "outputId": "b7bbed06-a0d0-4fee-ddfa-2776886592ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# compute sigmoid nonlinearity\n",
        "def sigmoid(x):\n",
        "    output = 1/(1+np.exp(-x))\n",
        "    return output\n",
        "\n",
        "# convert output of sigmoid function to its derivative\n",
        "def sigmoid_output_to_derivative(output):\n",
        "    return output*(1-output)\n",
        "    \n",
        "# input dataset\n",
        "X = np.array([  [0,1],\n",
        "                [0,1],\n",
        "                [1,0],\n",
        "                [1,0] ])\n",
        "    \n",
        "# output dataset            \n",
        "y = np.array([[0,0,1,1]]).T\n",
        "\n",
        "# seed random numbers to make calculation\n",
        "# deterministic (just a good practice)\n",
        "np.random.seed(1)\n",
        "\n",
        "# initialize weights randomly with mean 0\n",
        "synapse_0 = 2*np.random.random((2,1)) - 1\n",
        "\n",
        "for iter in range(10000):\n",
        "\n",
        "    # forward propagation\n",
        "    layer_0 = X\n",
        "    layer_1 = sigmoid(np.dot(layer_0,synapse_0))\n",
        "\n",
        "    # how much did we miss?\n",
        "    layer_1_error = layer_1 - y\n",
        "\n",
        "    # multiply how much we missed by the \n",
        "    # slope of the sigmoid at the values in l1\n",
        "    layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)\n",
        "    synapse_0_derivative = np.dot(layer_0.T,layer_1_delta)\n",
        "\n",
        "    # update weights\n",
        "    synapse_0 -= synapse_0_derivative\n",
        "\n",
        "print(\"Output After Training:\")\n",
        "print(layer_1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output After Training:\n",
            "[[0.00505119]\n",
            " [0.00505119]\n",
            " [0.99494905]\n",
            " [0.99494905]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFuu3Gyr74KX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9tWC0byeBEY",
        "colab_type": "code",
        "outputId": "4fd9ac91-469a-48a9-83ef-1db1dc46aef1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "#Challenge: find the best hyperparameters from alpha and hidden layer sizes\n",
        "#Bonus extend this to an artibray user enter number of hidden layers with user entered sizes\n",
        "alphas = [0.001,0.01,0.1,1,10,100,1000]\n",
        "hiddenSizes = [4,8,16,32]\n",
        "\n",
        "# compute sigmoid nonlinearity\n",
        "def sigmoid(x):\n",
        "    output = 1/(1+np.exp(-x))\n",
        "    return output\n",
        "\n",
        "# convert output of sigmoid function to its derivative\n",
        "def sigmoid_output_to_derivative(output):\n",
        "    return output*(1-output)\n",
        "    \n",
        "X = np.array([[0,0,1],\n",
        "            [0,1,1],\n",
        "            [1,0,1],\n",
        "            [1,1,1]])\n",
        "                \n",
        "y = np.array([[0],\n",
        "\t\t\t[1],\n",
        "\t\t\t[1],\n",
        "\t\t\t[0]])\n",
        "\n",
        "for alpha in alphas:\n",
        "  for hiddenSize in hiddenSizes:\n",
        "    print(\"\\nTraining With Alpha:\" + str(alpha) + \" and hidden layer size: \" + str(hiddenSize))\n",
        "    np.random.seed(1)\n",
        "\n",
        "    # randomly initialize our weights with mean 0\n",
        "    synapse_0 = 2*np.random.random((3,hiddenSize)) - 1\n",
        "    synapse_1 = 2*np.random.random((hiddenSize,1)) - 1\n",
        "\n",
        "    for j in range(60000):\n",
        "\n",
        "        # Feed forward through layers 0, 1, and 2\n",
        "        layer_0 = X\n",
        "        layer_1 = sigmoid(np.dot(layer_0,synapse_0))\n",
        "        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n",
        "\n",
        "        # how much did we miss the target value?\n",
        "        layer_2_error = layer_2 - y\n",
        "\n",
        "        if (j % 20000) == 0:\n",
        "            print(\"Error after \"+str(j)+\" iterations:\" + str(np.mean(np.abs(layer_2_error))))\n",
        "\n",
        "        # in what direction is the target value?\n",
        "        # were we really sure? if so, don't change too much.\n",
        "        layer_2_delta = layer_2_error*sigmoid_output_to_derivative(layer_2)\n",
        "\n",
        "        # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
        "        layer_1_error = layer_2_delta.dot(synapse_1.T)\n",
        "\n",
        "        # in what direction is the target l1?\n",
        "        # were we really sure? if so, don't change too much.\n",
        "        layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)\n",
        "\n",
        "        synapse_1 -= alpha * (layer_1.T.dot(layer_2_delta))\n",
        "        synapse_0 -= alpha * (layer_0.T.dot(layer_1_delta))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training With Alpha:0.001 and hidden layer size: 4\n",
            "Error after 0 iterations:0.4964100319027255\n",
            "Error after 20000 iterations:0.4935960431880486\n",
            "Error after 40000 iterations:0.48910016654420474\n",
            "\n",
            "Training With Alpha:0.001 and hidden layer size: 8\n",
            "Error after 0 iterations:0.49885891282661\n",
            "Error after 20000 iterations:0.4960339829421634\n",
            "Error after 40000 iterations:0.49230609024302613\n",
            "\n",
            "Training With Alpha:0.001 and hidden layer size: 16\n",
            "Error after 0 iterations:0.4968197940374576\n",
            "Error after 20000 iterations:0.4915701638369947\n",
            "Error after 40000 iterations:0.4835511920051555\n",
            "\n",
            "Training With Alpha:0.001 and hidden layer size: 32\n",
            "Error after 0 iterations:0.49643992250078794\n",
            "Error after 20000 iterations:0.48497630702745953\n",
            "Error after 40000 iterations:0.46903846539028254\n",
            "\n",
            "Training With Alpha:0.01 and hidden layer size: 4\n",
            "Error after 0 iterations:0.4964100319027255\n",
            "Error after 20000 iterations:0.359097202563399\n",
            "Error after 40000 iterations:0.14307065901337032\n",
            "\n",
            "Training With Alpha:0.01 and hidden layer size: 8\n",
            "Error after 0 iterations:0.49885891282661\n",
            "Error after 20000 iterations:0.33871559263379203\n",
            "Error after 40000 iterations:0.11085529399713788\n",
            "\n",
            "Training With Alpha:0.01 and hidden layer size: 16\n",
            "Error after 0 iterations:0.4968197940374576\n",
            "Error after 20000 iterations:0.2378006592135859\n",
            "Error after 40000 iterations:0.090147768216335\n",
            "\n",
            "Training With Alpha:0.01 and hidden layer size: 32\n",
            "Error after 0 iterations:0.49643992250078794\n",
            "Error after 20000 iterations:0.14693984546475994\n",
            "Error after 40000 iterations:0.06514781927504919\n",
            "\n",
            "Training With Alpha:0.1 and hidden layer size: 4\n",
            "Error after 0 iterations:0.4964100319027255\n",
            "Error after 20000 iterations:0.024098994228521613\n",
            "Error after 40000 iterations:0.014987616272210912\n",
            "\n",
            "Training With Alpha:0.1 and hidden layer size: 8\n",
            "Error after 0 iterations:0.49885891282661\n",
            "Error after 20000 iterations:0.024038515823849808\n",
            "Error after 40000 iterations:0.015353573121544378\n",
            "\n",
            "Training With Alpha:0.1 and hidden layer size: 16\n",
            "Error after 0 iterations:0.4968197940374576\n",
            "Error after 20000 iterations:0.02386891164981858\n",
            "Error after 40000 iterations:0.015406466690561896\n",
            "\n",
            "Training With Alpha:0.1 and hidden layer size: 32\n",
            "Error after 0 iterations:0.49643992250078794\n",
            "Error after 20000 iterations:0.019063872533418433\n",
            "Error after 40000 iterations:0.012389242990471293\n",
            "\n",
            "Training With Alpha:1 and hidden layer size: 4\n",
            "Error after 0 iterations:0.4964100319027255\n",
            "Error after 20000 iterations:0.0057894598625078085\n",
            "Error after 40000 iterations:0.0039587652802736475\n",
            "\n",
            "Training With Alpha:1 and hidden layer size: 8\n",
            "Error after 0 iterations:0.49885891282661\n",
            "Error after 20000 iterations:0.006064058766005624\n",
            "Error after 40000 iterations:0.0041540985823973085\n",
            "\n",
            "Training With Alpha:1 and hidden layer size: 16\n",
            "Error after 0 iterations:0.4968197940374576\n",
            "Error after 20000 iterations:0.006081688500988915\n",
            "Error after 40000 iterations:0.004147665245792007\n",
            "\n",
            "Training With Alpha:1 and hidden layer size: 32\n",
            "Error after 0 iterations:0.49643992250078794\n",
            "Error after 20000 iterations:0.004972517050388162\n",
            "Error after 40000 iterations:0.0033864102198316558\n",
            "\n",
            "Training With Alpha:10 and hidden layer size: 4\n",
            "Error after 0 iterations:0.4964100319027255\n",
            "Error after 20000 iterations:0.0021445955798521767\n",
            "Error after 40000 iterations:0.001478214512290799\n",
            "\n",
            "Training With Alpha:10 and hidden layer size: 8\n",
            "Error after 0 iterations:0.49885891282661\n",
            "Error after 20000 iterations:0.0015995429469240542\n",
            "Error after 40000 iterations:0.0011113112681715655\n",
            "\n",
            "Training With Alpha:10 and hidden layer size: 16\n",
            "Error after 0 iterations:0.4968197940374576\n",
            "Error after 20000 iterations:0.0016467187353623783\n",
            "Error after 40000 iterations:0.001143653434163994\n",
            "\n",
            "Training With Alpha:10 and hidden layer size: 32\n",
            "Error after 0 iterations:0.49643992250078794\n",
            "Error after 20000 iterations:0.001532635713923703\n",
            "Error after 40000 iterations:0.001054907732624855\n",
            "\n",
            "Training With Alpha:100 and hidden layer size: 4\n",
            "Error after 0 iterations:0.4964100319027255\n",
            "Error after 20000 iterations:0.12533033352910083\n",
            "Error after 40000 iterations:0.12523107366284103\n",
            "\n",
            "Training With Alpha:100 and hidden layer size: 8\n",
            "Error after 0 iterations:0.49885891282661\n",
            "Error after 20000 iterations:0.5000008759677884\n",
            "Error after 40000 iterations:0.5000009297529785\n",
            "\n",
            "Training With Alpha:100 and hidden layer size: 16\n",
            "Error after 0 iterations:0.4968197940374576\n",
            "Error after 20000 iterations:0.5\n",
            "Error after 40000 iterations:0.5\n",
            "\n",
            "Training With Alpha:100 and hidden layer size: 32\n",
            "Error after 0 iterations:0.49643992250078794\n",
            "Error after 20000 iterations:0.5\n",
            "Error after 40000 iterations:0.5\n",
            "\n",
            "Training With Alpha:1000 and hidden layer size: 4\n",
            "Error after 0 iterations:0.4964100319027255\n",
            "Error after 20000 iterations:0.5\n",
            "Error after 40000 iterations:0.5\n",
            "\n",
            "Training With Alpha:1000 and hidden layer size: 8\n",
            "Error after 0 iterations:0.49885891282661\n",
            "Error after 20000 iterations:0.5\n",
            "Error after 40000 iterations:0.5\n",
            "\n",
            "Training With Alpha:1000 and hidden layer size: 16\n",
            "Error after 0 iterations:0.4968197940374576\n",
            "Error after 20000 iterations:0.5\n",
            "Error after 40000 iterations:0.5\n",
            "\n",
            "Training With Alpha:1000 and hidden layer size: 32\n",
            "Error after 0 iterations:0.49643992250078794\n",
            "Error after 20000 iterations:0.5\n",
            "Error after 40000 iterations:0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ok14OdLE41KO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}